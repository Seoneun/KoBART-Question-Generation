{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36945cb3",
   "metadata": {},
   "source": [
    "# 필요 라이브러리 불러오기 및 Mecab 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134396cb",
   "metadata": {},
   "source": [
    " - Mecab과 학습에서 사용한 토크나이저를 사용하여 BLEU 스코어를 측정\n",
    " - Mecab으로 측정했을 때 성능이 좀 더 우수했음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e782595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# mecab이 설치된 경로를 넣어주세요\n",
    "mecab_path = 'C:\\mecab\\mecab-ko-dic'\n",
    "\n",
    "mecab = Mecab(mecab_path)\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('Sehong/kobart-QuestionGeneration')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ed0a48",
   "metadata": {},
   "source": [
    "# 데이터 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa36f840",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = 'data/dev.tsv' # .tsv\n",
    "output_path = 'output/output.txt' # .txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439789cd",
   "metadata": {},
   "source": [
    "# BLEU 스코어 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f512dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "\n",
    "# import language_evaluation\n",
    "from typing import List\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import math\n",
    "import sys\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "def mean(lst):\n",
    "    return sum(lst) / len(lst)\n",
    "\n",
    "\n",
    "def _calc_ngram_dict(tokens: List[str], ngram: int, dict_ref=None):\n",
    "    ngram_dict = defaultdict(int) if dict_ref is None else dict_ref\n",
    "    total = len(tokens)\n",
    "    for i in range(0, total - ngram + 1):\n",
    "        item = tuple(tokens[i:i + ngram])\n",
    "        ngram_dict[item] += 1\n",
    "    return ngram_dict\n",
    "\n",
    "\n",
    "def _calc_cover(cand, gold, ngram):\n",
    "    cand_dict = _calc_ngram_dict(cand, ngram)\n",
    "    gold_dict = _calc_ngram_dict(gold, ngram)\n",
    "    cover = 0\n",
    "    total = 0\n",
    "    for token, freq in cand_dict.items():\n",
    "        if token in gold_dict:\n",
    "            cover += min(freq, gold_dict[token])\n",
    "        total += freq\n",
    "    return cover, total\n",
    "\n",
    "\n",
    "def _calc_cover_rate(cands, golds, ngram):\n",
    "    \"\"\"\n",
    "    calc_cover_rate\n",
    "    \"\"\"\n",
    "    cover = 0.0\n",
    "    total = 0.000001\n",
    "    for cand_tokens, gold_tokens in zip(cands, golds):\n",
    "        cur_cover, cur_total = _calc_cover(cand_tokens, gold_tokens, ngram)\n",
    "        cover += cur_cover\n",
    "        total += cur_total\n",
    "    return cover / total\n",
    "\n",
    "\n",
    "def _calc_bp(cands, golds):\n",
    "    c_count = 0.000001\n",
    "    r_count = 0.0\n",
    "    for cand_tokens, gold_tokens in zip(cands, golds):\n",
    "        c_count += len(cand_tokens)\n",
    "        r_count += len(gold_tokens)\n",
    "    bp = 1\n",
    "    if c_count < r_count:\n",
    "        bp = math.exp(1 - r_count / c_count)\n",
    "    return bp\n",
    "\n",
    "\n",
    "def calc_corpus_bleu(cands, golds):\n",
    "    bp = _calc_bp(cands, golds)\n",
    "    cover_rate1 = _calc_cover_rate(cands, golds, 1)\n",
    "    cover_rate2 = _calc_cover_rate(cands, golds, 2)\n",
    "    cover_rate3 = _calc_cover_rate(cands, golds, 3)\n",
    "    bleu1 = 0\n",
    "    bleu2 = 0\n",
    "    bleu3 = 0\n",
    "    if cover_rate1 > 0:\n",
    "        bleu1 = bp * math.exp(math.log(cover_rate1))\n",
    "    if cover_rate2 > 0:\n",
    "        bleu2 = bp * math.exp((math.log(cover_rate1) + math.log(cover_rate2)) / 2)\n",
    "    if cover_rate3 > 0:\n",
    "        bleu3 = bp * math.exp((math.log(cover_rate1) + math.log(cover_rate2) + math.log(cover_rate3)) / 3)\n",
    "    return bleu1, bleu2, bleu3\n",
    "\n",
    "\n",
    "# def calc_corpus_bleu_new(cands, golds):\n",
    "#     golds = [[gold] for gold in golds]\n",
    "#     sf = SmoothingFunction().method7\n",
    "#     bleu1 = corpus_bleu(golds, cands, smoothing_function=sf, weights=[1, 0, 0, 0])\n",
    "#     bleu2 = corpus_bleu(golds, cands, smoothing_function=sf, weights=[0.5, 0.5, 0, 0])\n",
    "#     bleu3 = corpus_bleu(golds, cands, smoothing_function=sf, weights=[0.34, 0.33, 0.33, 0])\n",
    "#     return bleu1, bleu2, bleu3\n",
    "\n",
    "def calc_sentence_bleu(cands, golds):\n",
    "    bleu1 = []\n",
    "    bleu2 = []\n",
    "    bleu3 = []\n",
    "    sf = SmoothingFunction().method7\n",
    "    for hyp, ref in zip(cands, golds):\n",
    "        try:\n",
    "            b1 = sentence_bleu([ref], hyp, smoothing_function=sf, weights=[1, 0, 0, 0])\n",
    "        except ZeroDivisionError:\n",
    "            b1 = 0.0\n",
    "        try:\n",
    "            b2 = sentence_bleu([ref], hyp, smoothing_function=sf, weights=[0.5, 0.5, 0, 0])\n",
    "        except ZeroDivisionError:\n",
    "            b2 = 0.0\n",
    "        try:\n",
    "            b3 = sentence_bleu([ref], hyp, smoothing_function=sf, weights=[0.34, 0.33, 0.33, 0])\n",
    "        except ZeroDivisionError:\n",
    "            b3 = 0.0\n",
    "        bleu1.append(b1)\n",
    "        bleu2.append(b2)\n",
    "        bleu3.append(b3)\n",
    "    return mean(bleu1), mean(bleu2), mean(bleu3)\n",
    "\n",
    "\n",
    "def calc_corpus_bleu_new(hypothesis, references):\n",
    "    # hypothesis = [normalize_answer(hyp).split(\" \") for hyp in hypothesis]\n",
    "    # references = [[normalize_answer(ref).split(\" \")] for ref in references]\n",
    "    references = [[gold] for gold in references]\n",
    "    sf = SmoothingFunction(epsilon=1e-12).method1\n",
    "    b1 = corpus_bleu(references, hypothesis, weights=(1.0 / 1.0,), smoothing_function=sf)\n",
    "    b2 = corpus_bleu(references, hypothesis, weights=(1.0 / 2.0, 1.0 / 2.0), smoothing_function=sf)\n",
    "    b3 = corpus_bleu(references, hypothesis, weights=(1.0 / 3.0, 1.0 / 3.0, 1.0 / 3.0), smoothing_function=sf)\n",
    "    b4 = corpus_bleu(references, hypothesis, weights=(1.0 / 4.0, 1.0 / 4.0, 1.0 / 4.0, 1.0 / 4.0),\n",
    "                     smoothing_function=sf)\n",
    "    return b1, b2, b3, b4\n",
    "\n",
    "\n",
    "def _calc_distinct_ngram(cands, ngram):\n",
    "    ngram_total = 0.00001\n",
    "    ngram_distinct_count = 0.00001\n",
    "    pred_dict = defaultdict(int)\n",
    "    for cand_tokens in cands:\n",
    "        _calc_ngram_dict(cand_tokens, ngram, pred_dict)\n",
    "    for key, freq in pred_dict.items():\n",
    "        ngram_total += freq\n",
    "        ngram_distinct_count += 1\n",
    "    return ngram_distinct_count / ngram_total\n",
    "\n",
    "\n",
    "def _calc_sent_distinct_ngram(cand, ngram):\n",
    "    ngram_total = 0.0000000001\n",
    "    ngram_distinct_count = 0.0\n",
    "    ngram_dict = defaultdict(int)\n",
    "    for i in range(0, len(cand) - ngram + 1):\n",
    "        item = tuple(cand[i:i + ngram])\n",
    "        ngram_dict[item] += 1\n",
    "    for _, freq in ngram_dict.items():\n",
    "        ngram_total += freq\n",
    "        ngram_distinct_count += 1\n",
    "    return ngram_distinct_count / ngram_total\n",
    "\n",
    "\n",
    "def calc_corpus_distinct(cands):\n",
    "    distinct1 = _calc_distinct_ngram(cands, 1)\n",
    "    distinct2 = _calc_distinct_ngram(cands, 2)\n",
    "    return distinct1, distinct2\n",
    "\n",
    "\n",
    "def calc_sentence_distinct(cands):\n",
    "    distinct1 = mean([_calc_sent_distinct_ngram(c, 1) for c in cands])\n",
    "    distinct2 = mean([_calc_sent_distinct_ngram(c, 2) for c in cands])\n",
    "    return distinct1, distinct2\n",
    "\n",
    "\n",
    "def calc_corpus_f1(cands, golds):\n",
    "    golden_word_total = 0.00000001\n",
    "    pred_word_total = 0.00000001\n",
    "    hit_word_total = 0.00000001\n",
    "    for response, golden_response in zip(cands, golds):\n",
    "        common = Counter(response) & Counter(golden_response)\n",
    "        hit_word_total += sum(common.values())\n",
    "        golden_word_total += len(golden_response)\n",
    "        pred_word_total += len(response)\n",
    "    p = hit_word_total / pred_word_total\n",
    "    r = hit_word_total / golden_word_total\n",
    "    f1 = 2 * p * r / (p + r)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    re_art = re.compile(r'\\b(a|an|the)\\b')\n",
    "    re_punc = re.compile(r'[!\"#$%&()*+,-./:;<=>?@\\[\\]\\\\^`{|}~_\\']')\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re_art.sub(' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        return re_punc.sub(' ', text)  # convert punctuation to spaces\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s)))).split(' ')\n",
    "\n",
    "\n",
    "def calc_rouge(cands, golds):\n",
    "    rouge_evaluator = language_evaluation.RougeEvaluator(num_parallel_calls=1, tokenization_fn=normalize_answer)\n",
    "    predictions = [' '.join(c) for c in cands]\n",
    "    answers = [' '.join(g) for g in golds]\n",
    "    rouge_result = rouge_evaluator.run_evaluation(predictions, answers)\n",
    "    return rouge_result\n",
    "\n",
    "\n",
    "def dialogue_evaluation(ori_cands, ori_golds):\n",
    "    assert len(ori_cands) == len(ori_golds), f\"num cand: {len(ori_cands)}, num gold: {len(ori_golds)}\"\n",
    "    cands = []\n",
    "    golds = []\n",
    "    \n",
    "    # 둘 중 원하는 토크나이저로 성능 평가하세요\n",
    "    \n",
    "    help_tokenize = lambda x: mecab.morphs(x.lower())         # Mecab\n",
    "    #help_tokenize = lambda x: tokenizer.encode(x.lower())     # 학습에 사용한 토크나이저\n",
    "    \n",
    "    for cand, gold in zip(ori_cands, ori_golds):\n",
    "        cands.append(help_tokenize(str(cand).lower()))\n",
    "        golds.append(help_tokenize(str(gold).lower()))\n",
    "    cbleu1, cbleu2, cbleu3, cbleu4 = calc_corpus_bleu_new(cands, golds)\n",
    "    sbleu1, sbleu2, sbleu3 = calc_sentence_bleu(cands, golds)\n",
    "    cdiv1, cdiv2 = calc_corpus_distinct(cands)\n",
    "    sdiv1, sdiv2 = calc_sentence_distinct(cands)\n",
    "    cf1 = calc_corpus_f1(cands, golds)\n",
    "    # rouge_result = calc_rouge(cands, golds)\n",
    "    result = {\n",
    "        'cf1': cf1,\n",
    "        'bleu1': cbleu1,\n",
    "        'bleu2': cbleu2,\n",
    "        'bleu3': cbleu3,\n",
    "        'bleu4': cbleu4,\n",
    "        'dist1': cdiv1,\n",
    "        'dist2': cdiv2,\n",
    "    }\n",
    "    # result.update(rouge_result)\n",
    "    result = {k: round(100 * v, 6) for k, v in result.items()}\n",
    "    return result\n",
    "\n",
    "\n",
    "def file_dialogue_evaluation(cand_file, gold_file):\n",
    "    print(f\"cand file: {cand_file}, gold file: {gold_file}\")\n",
    "    cands = []\n",
    "    golds = []\n",
    "    with open(cand_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            cands.append(line.strip())\n",
    "    with open(gold_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            golds.append(line.strip())\n",
    "    results = dialogue_evaluation(cands, golds)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9b4a61",
   "metadata": {},
   "source": [
    "# BLEU 스코어 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc319d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(test_data_path, encoding='cp949', sep='\\t')\n",
    "question = list(df['question'])\n",
    "\n",
    "generated_question = []\n",
    "\n",
    "with open(output_path, encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        generated_question.append(line)\n",
    "\n",
    "len_question = len(question)\n",
    "\n",
    "score_list_1 = np.zeros(len_question)\n",
    "score_list_2 = np.zeros(len_question)\n",
    "score_list_3 = np.zeros(len_question)\n",
    "score_list_4 = np.zeros(len_question)\n",
    "\n",
    "for i in range(len_question):\n",
    "    \n",
    "    matrix = dialogue_evaluation([generated_question[i]], [question[i]])\n",
    "    score_list_1[i] = matrix['bleu1']\n",
    "    score_list_2[i] = matrix['bleu2']\n",
    "    score_list_3[i] = matrix['bleu3']\n",
    "    score_list_4[i] = matrix['bleu4']\n",
    "    \n",
    "\n",
    "#score_list = np.array(score_list)\n",
    "print('bleu1: ', np.mean(score_list_1))\n",
    "print('bleu2: ', np.mean(score_list_2))\n",
    "print('bleu3: ', np.mean(score_list_3))\n",
    "print('bleu4: ', np.mean(score_list_4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
